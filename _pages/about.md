---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hai-Tao Wu is now a second-year graduate student at Tianjin University under the supervision of Professor Changqing Zhang. He has received systematic learning in artificial intelligence and computer science, including
machine learning, computer vision, natural language processing, and speech information processing. He is self-motivated, has diverse learning interests, and strives for deep analytical thinking in the field of multimodal. He is currently working on research related to multimodal fusion and alignment, as well as reasoning and evaluation in large language models. 
<!-- I have published at CVPR, ICML conference with total <a href='https://scholar.google.com/citations?user=MawuZZUAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>. -->
He has published papers at top CV and ML conferences such as CVPR and ICML, with a total of <strong>101</strong> Google Scholar citations.


# ğŸ”¥ News
- *2025.04*: &nbsp;ğŸ‰ğŸ‰ We release two preprint about LLM Reasoning.
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ One paper about multimodal alignment accepted by CVPR 2025, thanks to all co-authors!
- *2024.12*: &nbsp;ğŸ‰ğŸ‰ Starting an internship at Shanghai AI lab, mentored by Huaxi Huang.
- *2023.04*: &nbsp;ğŸ‰ğŸ‰ One paper about multimodal fusion accepted by ICML 2023, thanks to all co-authors!

<h1>
  <img src="images/arxiv-logomark-small.svg" id="preprint" alt="arXiv Logo" style="vertical-align: middle; height: 1.5em; margin-right: 0.4em;">
  Preprint
</h1>
<!-- # ğŸ“ Preprint -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv:2504.20771</div><img src='images/TMBench_cover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Computational Reasoning of Large Language Models](https://arxiv.org/abs/2504.20771)

**Haitao Wu**, Zongbo Han, Joey Tianyi Zhou, Huaxi Huang, Changqing Zhang

[[Paper](https://arxiv.org/abs/2504.20771)] [[Code](https://github.com/HaitaoWuTJU/Turing-Machine-Bench)]

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv:2504.05812</div><img src='images/EMPO_cover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization](https://arxiv.org/abs/2504.05812)

Qingyang Zhang, **Haitao Wu**, Changqing Zhang, Peilin Zhao, Yatao Bian

[[Paper](https://arxiv.org/abs/2504.05812)] [[Code](https://github.com/QingyangZhang/EMPO)]

</div>
</div>


# ğŸ“ Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/UBP_cover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior
](https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Bridging_the_Vision-Brain_Gap_with_an_Uncertainty-Aware_Blur_Prior_CVPR_2025_paper.pdf)

**Haitao Wu**, Qing Li, Changqing Zhang, Zhen He, Xiaomin Ying

[[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Bridging_the_Vision-Brain_Gap_with_an_Uncertainty-Aware_Blur_Prior_CVPR_2025_paper.pdf)] [[Code](https://github.com/HaitaoWuTJU/Uncertainty-aware-Blur-Prior)]
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/QMF_cover.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Provable Dynamic Fusion for Low-quality Multimodal Learning
](https://proceedings.mlr.press/v202/zhang23ar/zhang23ar.pdf)

Qingyang Zhang, **Haitao Wu**, Changqing Zhang, Qinghua Hu, Huazhu Fu, Joey Tianyi Zhou, Xi Peng

[[Paper](https://proceedings.mlr.press/v202/zhang23ar/zhang23ar.pdf)] [[Code](https://github.com/QingyangZhang/QMF)]

<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
</div>
</div>

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

# Honors and Awards
- *2022.09* National Scholarship


# ğŸ“– Educations
- *2023.09 - Now*, Master of Science in Computer Science, Tianjin University
- *2019.09 - 2023.06*, Bachelor of Science in Artificial Intelligence, Tianjin University

<!-- # ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# ğŸ’» Internships
- *2024.12 - Now*, Shanghai AI Lab, China.
- *2021.9 - 2021.12*, Baidu, China.